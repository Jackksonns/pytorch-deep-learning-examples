import os
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torchvision import transforms, datasets, models
from PIL import Image
import matplotlib.pyplot as plt
import tarfile
import urllib.request
import ssl

def download_voc2012_from_mirror(target_dir='VOCdevkit'):
    voc_root = os.path.join(target_dir, 'VOC2012')
    if os.path.isdir(voc_root):
        print(f"[âœ”] å·²å­˜åœ¨ï¼š{voc_root}")
        return

    url = 'http://data.brainchip.com/dataset-mirror/voc/VOCtrainval_11-May-2012.tar'
    filename = 'VOCtrainval_11-May-2012.tar'
    download_path = os.path.join(target_dir, filename)
    os.makedirs(target_dir, exist_ok=True)

    if not os.path.exists(download_path):
        print(f"[â†“] æ­£åœ¨ä¸‹è½½ï¼š{url}")
        urllib.request.urlretrieve(url, download_path)
        print(f"[âœ”] ä¸‹è½½åˆ°ï¼š{download_path}")
    else:
        print(f"[!] å‹ç¼©åŒ…å·²å­˜åœ¨ï¼š{download_path}")

    print(f"[ğŸ“¦] è§£å‹åˆ° {target_dir} â€¦")
    with tarfile.open(download_path) as tar:
        tar.extractall(path=target_dir)
    print("[âœ”] è§£å‹å®Œæˆï¼")


# å…¨å·ç§¯ç½‘ç»œï¼ˆå…¨è¿æ¥å·ç§¯ç¥ç»ç½‘ç»œ-FCNï¼‰
# æ­å»ºæ¨¡å‹
# ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet18å»æ‰æœ€åçš„å…¨è¿æ¥å±‚å’Œå¹³å‡æ± åŒ–å±‚
#ä½¿ç”¨åœ¨ImageNetæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ResNet-18ä½œä¸ºæºæ¨¡å‹ã€‚â€”â€”æå–å›¾åƒç‰¹å¾
pretrained_net = models.resnet18(pretrained=True)
# æŸ¥çœ‹åŸç½‘ç»œæœ€å3å±‚ç»“æ„
print(list(pretrained_net.children())[-3:])

# åˆ›å»ºä¸€ä¸ªå…¨å·ç§¯ç½‘ç»œnetï¼Œä¿ç•™åˆ°å€’æ•°ç¬¬3å±‚ä¹‹å‰çš„æ‰€æœ‰å±‚
net = nn.Sequential(*list(pretrained_net.children())[:-2])

# æµ‹è¯•è¾“å‡ºå½¢çŠ¶
X = torch.rand(size=(1, 3, 320, 480))
print('feature map shape:', net(X).shape)  # (1,512,10,15)

# ä½¿ç”¨1x1å·ç§¯å°†è¾“å‡ºé€šé“æ•°è½¬æ¢ä¸ºVOCæ•°æ®é›†çš„ç±»åˆ«æ•°
num_classes = 21  # VOCä¸­åŒ…æ‹¬èƒŒæ™¯å…±21ç±»
net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))
# è½¬ç½®å·ç§¯ç”¨äºä¸Šé‡‡æ ·ï¼Œå°†ç‰¹å¾å›¾æ”¾å¤§32å€
net.add_module('transpose_conv', nn.ConvTranspose2d(
    num_classes, num_classes, kernel_size=64, padding=16, stride=32))

# åˆå§‹åŒ–è½¬ç½®å·ç§¯å±‚ä¸ºåŒçº¿æ€§æ’å€¼æƒé‡
# åŒçº¿æ€§æ’å€¼çš„ä¸Šé‡‡æ ·å¯é€šè¿‡è½¬ç½®å·ç§¯å±‚å®ç°

def bilinear_kernel(in_channels, out_channels, kernel_size):
    factor = (kernel_size + 1) // 2
    if kernel_size % 2 == 1:
        center = factor - 1
    else:
        center = factor - 0.5
    og_x = torch.arange(kernel_size).reshape(-1, 1)
    og_y = torch.arange(kernel_size).reshape(1, -1)
    filt = (1 - torch.abs(og_x - center) / factor) * \
           (1 - torch.abs(og_y - center) / factor)
    weight = torch.zeros((in_channels, out_channels, kernel_size, kernel_size))
    # å°†filtèµ‹å€¼åˆ°å¯¹åº”çš„å¯¹è§’ä½ç½®
    for i in range(min(in_channels, out_channels)):
        weight[i, i, :, :] = filt
    return weight

# ç”¨äºæµ‹è¯•åŒçº¿æ€§æƒé‡çš„è½¬ç½®å·ç§¯ï¼Œstride=2å°±æ˜¯é«˜å®½æ”¾å¤§ä¸¤å€
conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2, bias=False)
conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4))

# åŠ è½½å¹¶æµ‹è¯•ä¸Šé‡‡æ ·æ•ˆæœ
# è¯»å–å›¾åƒå¹¶è½¬æ¢ä¸ºTensor
img = transforms.ToTensor()(Image.open('./catdog.jpg'))  # ä¿ç•™åŸæ³¨é‡Šè·¯å¾„
X = img.unsqueeze(0)  # å¢åŠ æ‰¹æ¬¡ç»´åº¦
Y = conv_trans(X)
out_img = Y[0].permute(1, 2, 0).detach()

# æ˜¾ç¤ºè¾“å…¥å’Œè¾“å‡ºå›¾åƒ
print('input image shape:', img.permute(1, 2, 0).shape)
plt.imshow(img.permute(1, 2, 0))
plt.show()
plt.savefig('image1.png')
print('output image shape:', out_img.shape)
plt.imshow(out_img)
plt.show()
plt.savefig('image2.png')

# ç”¨åŒçº¿æ€§æ’å€¼åˆå§‹åŒ–FCNä¸­çš„transpose_conv
W = bilinear_kernel(num_classes, num_classes, 64)
net.transpose_conv.weight.data.copy_(W)

# æ•°æ®é›†å‡†å¤‡
# ä½¿ç”¨torchvisionçš„VOCSegmentationæ¥åŠ è½½VOC2012åˆ†å‰²æ•°æ®é›†
# å®šä¹‰å›¾åƒå’Œæ ‡ç­¾çš„é¢„å¤„ç†
normalize_mean = [0.485, 0.456, 0.406]
normalize_std = [0.229, 0.224, 0.225]

def get_voc_dataloader(batch_size, crop_size, root='VOCdevkit'):
    # è®­ç»ƒé›†å˜æ¢ï¼šéšæœºè£å‰ªã€è½¬Tensorã€å½’ä¸€åŒ–
    train_transform = transforms.Compose([
        transforms.RandomCrop(crop_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=normalize_mean, std=normalize_std)
    ])
    # éªŒè¯/æµ‹è¯•é›†å˜æ¢ï¼šä¸­å¿ƒè£å‰ªã€è½¬Tensorã€å½’ä¸€åŒ–
    val_transform = transforms.Compose([
        transforms.CenterCrop(crop_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=normalize_mean, std=normalize_std)
    ])
    # æ ‡ç­¾ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼ä¿æŒæ•´æ•°ç±»åˆ«
    target_transform = transforms.Compose([
        transforms.RandomCrop(crop_size),
        transforms.PILToTensor()
    ])
    train_ds = datasets.VOCSegmentation(root, year='2012', image_set='train', download=False,
                                       transform=train_transform,
                                       target_transform=target_transform)
    val_ds = datasets.VOCSegmentation(root, year='2012', image_set='val', download=False,
                                     transform=val_transform,
                                     target_transform=target_transform)
    train_loader = torch.utils.data.DataLoader(train_ds, batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_ds, batch_size, shuffle=False)
    return train_loader, val_loader

download_voc2012_from_mirror('VOCdevkit')

batch_size, crop_size = 32, (320, 480)
train_iter, test_iter = get_voc_dataloader(batch_size, crop_size)

#è®­ç»ƒä¸è¯„ä¼°
# å®šä¹‰æŸå¤±å‡½æ•°
# reduction='none'è¿”å›æ¯åƒç´ æŸå¤±ï¼Œåç»­å¯¹æ¯å¼ å›¾åƒå–å¹³å‡

def loss_fn(inputs, targets):
    # å»æ‰é€šé“ç»´åº¦ç¡®ä¿targetså½¢çŠ¶[N,H,W]
    targets = targets.squeeze(1).long()
    # äº¤å‰ç†µæŸå¤±é»˜è®¤å¯¹channelç»´åº¦åšsoftmax
    return F.cross_entropy(inputs, targets, reduction='mean')

# è®¾å¤‡é…ç½®
devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())] or [torch.device('cpu')]
print('training on:', devices)

# ä¼˜åŒ–å™¨
trainer = torch.optim.SGD(net.parameters(), lr=0.001, weight_decay=1e-3)

# å‡†ç¡®ç‡è®¡ç®—

def evaluate_accuracy(net, data_loader, device):
    net.eval()
    metric = {'loss': 0.0, 'num': 0}
    with torch.no_grad():
        for X, y in data_loader:
            X, y = X.to(device), y.to(device)
            outputs = net(X)
            l = loss_fn(outputs, y)
            metric['loss'] += l.item() * X.shape[0]
            metric['num'] += X.shape[0]
    return metric['loss'] / metric['num']

# è®­ç»ƒå‡½æ•°

def train(net, train_loader, val_loader, loss_fn, trainer, num_epochs, devices):
    for epoch in range(num_epochs):
        net.train()
        total_loss, total_num = 0.0, 0
        for X, y in train_loader:
            X, y = X.to(devices[0]), y.to(devices[0])
            trainer.zero_grad()
            outputs = net(X)
            l = loss_fn(outputs, y)
            l.backward()
            trainer.step()
            total_loss += l.item() * X.shape[0]
            total_num += X.shape[0]
        train_loss = total_loss / total_num
        val_loss = evaluate_accuracy(net, val_loader, devices[0])
        print(f"Epoch {epoch+1}/{num_epochs}, Train loss: {train_loss:.4f}, "
              f"Val loss: {val_loss:.4f}")

# å¼€å§‹è®­ç»ƒ
num_epochs = 5
train(net, train_iter, test_iter, loss_fn, trainer, num_epochs, devices)

#é¢„æµ‹ä¸å¯è§†åŒ–

def predict(img, net, device):
    # å›¾åƒå½’ä¸€åŒ–å¹¶å¢åŠ batchç»´åº¦
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=normalize_mean, std=normalize_std)
    ])
    X = transform(img).unsqueeze(0).to(device)
    pred = net(X).argmax(dim=1)
    #åœ¨é€šé“ç»´åº¦åšargmax
    return pred[0].cpu()

# VOCé¢œè‰²æ˜ å°„è¡¨
VOC_COLORMAP = [[0,0,0], [128,0,0], [0,128,0], [128,128,0], [0,0,128],
                [128,0,128], [0,128,128], [128,128,128], [64,0,0], [192,0,0],
                [64,128,0], [192,128,0], [64,0,128], [192,0,128], [64,128,128],
                [192,128,128], [0,64,0], [128,64,0], [0,192,0], [128,192,0],
                [0,64,128]]

# å°†é¢„æµ‹æ ‡ç­¾æ˜ å°„ä¸ºRGBå›¾åƒ

def label2image(pred):
    colormap = torch.tensor(VOC_COLORMAP)
    return colormap[pred.numpy()]

# ä»VOCæ•°æ®é›†ä¸­è¯»å–æµ‹è¯•å›¾åƒå¹¶å±•ç¤ºé¢„æµ‹ç»“æœ
voc_root = 'VOCdevkit'
if not os.path.isdir(voc_root):
    raise FileNotFoundError(f"ç›®å½• {voc_root} æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å·²ä¸‹è½½VOC2012æ•°æ®é›†ã€‚")

# éšæœºé€‰å–nå¼ å±•ç¤º
n = 4
fig, axes = plt.subplots(3, n, figsize=(n*3, 9))
for i in range(n):
    img, label = test_iter.dataset[i]
    pil_img = transforms.ToPILImage()(img)
    # é¢„æµ‹ç»“æœ
    pred = predict(pil_img, net, devices[0])
    # å¯è§†åŒ–åŸå›¾ã€é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
    axes[0, i].imshow(pil_img)
    axes[0, i].set_title('åŸå›¾')
    axes[1, i].imshow(label2image(pred))
    axes[1, i].set_title('é¢„æµ‹')
    axes[2, i].imshow(transforms.ToPILImage()(label.squeeze(0)))
    axes[2, i].set_title('çœŸå®')
    for ax in axes[:, i]:
        ax.axis('off')
plt.tight_layout()
plt.show()
plt.savefig('image3.png')
